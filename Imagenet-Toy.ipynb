{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading code\n",
    "\n",
    "##!!!! change the valdir to your data path\n",
    "valdir = '/data/public/imagenet2012/val'   \n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "batch_size = 256\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(valdir, transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time, losses, top1, top5],\n",
    "        prefix='Test: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "           \n",
    "            images = images.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "        # TODO: this should also be done with the ProgressMeter\n",
    "        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "              .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "    \n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arnold and iArnold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def arnold(img, key):\n",
    "    N, a, b, c, d = key  #  key=[1,1,1,1,2]\n",
    "    h, w = img.shape[: 2]\n",
    "    new_img = copy.deepcopy(img)\n",
    "\n",
    "    for i in range(N):\n",
    "        \n",
    "        for x in range(h):\n",
    "            \n",
    "            for y in range(w):\n",
    "                \n",
    "                nx = (a * x + b * y) % w\n",
    "                ny = (c * x + d * y) % w \n",
    "                nx = int(nx)\n",
    "                \n",
    "                ny = int(ny)\n",
    "                \n",
    "                new_img[nx, ny] = img[x, y]\n",
    "        img = copy.deepcopy(new_img)\n",
    "    return new_img\n",
    "\n",
    "\n",
    "def iarnold(img, key):\n",
    "    N, a, b, c, d = key\n",
    "\n",
    "    #matrix = np.mat([[a, b], [c, d]]).I\n",
    "\n",
    "    [[a, b], [c, d]] = [[(a*b+1),-a],[-b,1]]\n",
    "    return arnold(img, [N, a, b, c, d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [  0/196]\tTime  2.319 ( 2.319)\tLoss 1.1817e+00 (1.1817e+00)\tAcc@1  68.75 ( 68.75)\tAcc@5  89.06 ( 89.06)\n",
      "Test: [ 10/196]\tTime  0.167 ( 1.091)\tLoss 1.8619e+00 (1.5258e+00)\tAcc@1  53.91 ( 64.49)\tAcc@5  80.08 ( 84.23)\n",
      "Test: [ 20/196]\tTime  1.476 ( 1.297)\tLoss 1.4823e+00 (1.4910e+00)\tAcc@1  67.19 ( 65.23)\tAcc@5  83.20 ( 84.77)\n",
      "Test: [ 30/196]\tTime  0.021 ( 1.475)\tLoss 1.3381e+00 (1.4429e+00)\tAcc@1  62.89 ( 66.15)\tAcc@5  89.45 ( 85.37)\n",
      "Test: [ 40/196]\tTime  0.886 ( 1.473)\tLoss 1.6524e+00 (1.5115e+00)\tAcc@1  57.81 ( 63.71)\tAcc@5  84.38 ( 84.90)\n",
      "Test: [ 50/196]\tTime  0.021 ( 1.559)\tLoss 1.0528e+00 (1.4948e+00)\tAcc@1  71.48 ( 63.26)\tAcc@5  92.58 ( 85.52)\n",
      "Test: [ 60/196]\tTime  0.021 ( 1.531)\tLoss 1.8547e+00 (1.4879e+00)\tAcc@1  54.30 ( 63.28)\tAcc@5  81.64 ( 85.79)\n",
      "Test: [ 70/196]\tTime  0.021 ( 1.616)\tLoss 1.5965e+00 (1.4681e+00)\tAcc@1  60.55 ( 63.75)\tAcc@5  84.77 ( 86.03)\n",
      "Test: [ 80/196]\tTime  0.021 ( 1.597)\tLoss 2.7413e+00 (1.4955e+00)\tAcc@1  41.02 ( 63.38)\tAcc@5  66.41 ( 85.62)\n",
      "Test: [ 90/196]\tTime  0.021 ( 1.647)\tLoss 3.1913e+00 (1.5896e+00)\tAcc@1  34.77 ( 61.77)\tAcc@5  61.33 ( 84.19)\n",
      "Test: [100/196]\tTime  0.021 ( 1.628)\tLoss 2.8773e+00 (1.6689e+00)\tAcc@1  36.72 ( 60.31)\tAcc@5  63.67 ( 83.03)\n",
      "Test: [110/196]\tTime  0.023 ( 1.677)\tLoss 1.7661e+00 (1.7036e+00)\tAcc@1  61.72 ( 59.78)\tAcc@5  78.52 ( 82.42)\n",
      "Test: [120/196]\tTime  0.021 ( 1.654)\tLoss 2.7229e+00 (1.7423e+00)\tAcc@1  42.58 ( 59.23)\tAcc@5  64.84 ( 81.78)\n",
      "Test: [130/196]\tTime  0.021 ( 1.680)\tLoss 1.7960e+00 (1.7856e+00)\tAcc@1  56.25 ( 58.43)\tAcc@5  79.30 ( 81.13)\n",
      "Test: [140/196]\tTime  0.021 ( 1.652)\tLoss 2.2469e+00 (1.8191e+00)\tAcc@1  48.05 ( 57.92)\tAcc@5  72.66 ( 80.56)\n",
      "Test: [150/196]\tTime  0.070 ( 1.665)\tLoss 2.5929e+00 (1.8588e+00)\tAcc@1  53.52 ( 57.43)\tAcc@5  70.70 ( 79.92)\n",
      "Test: [160/196]\tTime  0.021 ( 1.644)\tLoss 1.8736e+00 (1.8887e+00)\tAcc@1  57.42 ( 56.93)\tAcc@5  78.12 ( 79.41)\n",
      "Test: [170/196]\tTime  0.183 ( 1.663)\tLoss 1.2882e+00 (1.9154e+00)\tAcc@1  69.14 ( 56.46)\tAcc@5  88.67 ( 79.03)\n",
      "Test: [180/196]\tTime  0.021 ( 1.646)\tLoss 1.8957e+00 (1.9347e+00)\tAcc@1  49.61 ( 56.10)\tAcc@5  79.30 ( 78.71)\n",
      "Test: [190/196]\tTime  0.797 ( 1.659)\tLoss 1.6515e+00 (1.9248e+00)\tAcc@1  54.69 ( 56.22)\tAcc@5  85.55 ( 78.89)\n",
      " * Acc@1 56.520 Acc@5 79.068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(56.5200, device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "cudnn.benchmark = True\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "# resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "# vgg16 = models.vgg16(pretrained=True)\n",
    "#densenet = models.densenet121(pretrained=True)\n",
    "#inception = models.inception_v3(pretrained=True)\n",
    "# googlenet = models.googlenet(pretrained=True)\n",
    "#shufflenet = models.shufflenet_v2_x1_0(pretrained=True)\n",
    "#mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "\n",
    "# test the original accuracy\n",
    "alexnet = alexnet.cuda()\n",
    "validate(val_loader, alexnet, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see model structure\n",
    "alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight      torch.Size([64, 3, 11, 11])\n",
      "features.0.bias      torch.Size([64])\n",
      "features.3.weight      torch.Size([192, 64, 5, 5])\n",
      "features.3.bias      torch.Size([192])\n",
      "features.6.weight      torch.Size([384, 192, 3, 3])\n",
      "features.6.bias      torch.Size([384])\n",
      "features.8.weight      torch.Size([256, 384, 3, 3])\n",
      "features.8.bias      torch.Size([256])\n",
      "features.10.weight      torch.Size([256, 256, 3, 3])\n",
      "features.10.bias      torch.Size([256])\n",
      "classifier.1.weight      torch.Size([4096, 9216])\n",
      "classifier.1.bias      torch.Size([4096])\n",
      "classifier.4.weight      torch.Size([4096, 4096])\n",
      "classifier.4.bias      torch.Size([4096])\n",
      "classifier.6.weight      torch.Size([1000, 4096])\n",
      "classifier.6.bias      torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "cudnn.benchmark = True\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# change this model\n",
    "model = models.alexnet(pretrained=True)\n",
    "# model = models.googlenet(pretrained=True)\n",
    "#model = models.resnet101(pretrained=True)\n",
    "#model = models.mobilenet_v2(pretrained=True)\n",
    "#model = models.densenet121(pretrained=True)\n",
    "#model = models.vgg16(pretrained=True)\n",
    "model = model.cuda()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name,'    ', param.size())\n",
    "    \n",
    "# AlexNet\n",
    "# weight = model.features[0].weight\n",
    "# weight = model.features[3].weight\n",
    "# weight = model.features[6].weight\n",
    "# weight = model.features[8].weight\n",
    "# weight = model.features[10].weight\n",
    "# weight = model.classifier[1].weight\n",
    "\n",
    "\n",
    "# weight = model.inception4a.branch1.conv.weight\n",
    "#weight = model.conv3.conv.weight \n",
    "# print('layer weights size is: ',weight.size())\n",
    "# test original model acc\n",
    "#validate(val_loader, model, criterion) \n",
    "# print(weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer half weights size is:  torch.Size([64, 64, 5, 5])\n",
      "need 0.004524 sec\n",
      "Test: [  0/196]\tTime  3.280 ( 3.280)\tLoss 4.0747e+00 (4.0747e+00)\tAcc@1  24.61 ( 24.61)\tAcc@5  45.70 ( 45.70)\n",
      "Test: [ 10/196]\tTime  0.021 ( 0.998)\tLoss 4.0882e+00 (4.2193e+00)\tAcc@1  18.75 ( 20.53)\tAcc@5  46.48 ( 41.12)\n",
      "Test: [ 20/196]\tTime  1.567 ( 0.994)\tLoss 5.1035e+00 (3.9342e+00)\tAcc@1  14.45 ( 24.42)\tAcc@5  27.73 ( 45.61)\n",
      "Test: [ 30/196]\tTime  0.021 ( 0.971)\tLoss 4.2868e+00 (3.9508e+00)\tAcc@1  15.23 ( 24.40)\tAcc@5  38.67 ( 45.17)\n",
      "Test: [ 40/196]\tTime  0.358 ( 0.920)\tLoss 4.7175e+00 (4.2960e+00)\tAcc@1   9.38 ( 20.44)\tAcc@5  30.08 ( 39.35)\n",
      "Test: [ 50/196]\tTime  0.021 ( 0.931)\tLoss 3.4557e+00 (4.4173e+00)\tAcc@1  25.39 ( 18.85)\tAcc@5  47.27 ( 37.40)\n",
      "Test: [ 60/196]\tTime  0.021 ( 0.896)\tLoss 3.7440e+00 (4.3694e+00)\tAcc@1  18.36 ( 18.73)\tAcc@5  45.70 ( 38.10)\n",
      "Test: [ 70/196]\tTime  0.021 ( 0.903)\tLoss 4.5519e+00 (4.3016e+00)\tAcc@1  19.53 ( 19.66)\tAcc@5  36.72 ( 39.39)\n",
      "Test: [ 80/196]\tTime  0.021 ( 0.876)\tLoss 4.7752e+00 (4.2567e+00)\tAcc@1  10.55 ( 20.21)\tAcc@5  27.73 ( 40.02)\n",
      "Test: [ 90/196]\tTime  0.260 ( 0.885)\tLoss 4.4925e+00 (4.2408e+00)\tAcc@1  15.23 ( 20.26)\tAcc@5  33.20 ( 40.13)\n",
      "Test: [100/196]\tTime  0.031 ( 0.862)\tLoss 4.9075e+00 (4.2392e+00)\tAcc@1  10.55 ( 20.10)\tAcc@5  26.95 ( 40.08)\n",
      "Test: [110/196]\tTime  0.673 ( 0.871)\tLoss 3.6974e+00 (4.2256e+00)\tAcc@1  32.81 ( 20.28)\tAcc@5  46.88 ( 40.27)\n",
      "Test: [120/196]\tTime  0.036 ( 0.855)\tLoss 4.3441e+00 (4.2120e+00)\tAcc@1  19.14 ( 20.50)\tAcc@5  36.72 ( 40.52)\n",
      "Test: [130/196]\tTime  1.658 ( 0.865)\tLoss 4.2798e+00 (4.2088e+00)\tAcc@1  19.92 ( 20.38)\tAcc@5  37.11 ( 40.43)\n",
      "Test: [140/196]\tTime  0.042 ( 0.852)\tLoss 3.3622e+00 (4.1933e+00)\tAcc@1  32.03 ( 20.51)\tAcc@5  58.59 ( 40.72)\n",
      "Test: [150/196]\tTime  2.780 ( 0.860)\tLoss 3.7342e+00 (4.1862e+00)\tAcc@1  28.91 ( 20.62)\tAcc@5  45.70 ( 40.74)\n",
      "Test: [160/196]\tTime  0.021 ( 0.849)\tLoss 4.4397e+00 (4.1751e+00)\tAcc@1  18.75 ( 20.75)\tAcc@5  35.94 ( 40.92)\n",
      "Test: [170/196]\tTime  3.276 ( 0.856)\tLoss 3.6707e+00 (4.1753e+00)\tAcc@1  28.52 ( 20.69)\tAcc@5  53.12 ( 40.88)\n",
      "Test: [180/196]\tTime  0.021 ( 0.844)\tLoss 3.7036e+00 (4.1549e+00)\tAcc@1  27.73 ( 20.98)\tAcc@5  48.44 ( 41.24)\n",
      "Test: [190/196]\tTime  2.862 ( 0.850)\tLoss 4.0031e+00 (4.1389e+00)\tAcc@1  20.31 ( 21.02)\tAcc@5  48.44 ( 41.48)\n",
      " * Acc@1 21.168 Acc@5 41.630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(21.1680, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Choose one layer (i.e., layer 3, namely features.3.weight)\n",
    "weight = model.features[3].weight\n",
    "weight_half = weight[:64,:64]\n",
    "print('layer half weights size is: ',weight_half.size())\n",
    "weight_numpy = weight_half.detach().cpu().numpy()\n",
    "start = time.time()\n",
    "\n",
    "# secret key is [1,1,1,1,2]   \\tau = 1  p=1, q=1\n",
    "arn_weight = arnold(weight_numpy,[1, 1,1,1,2])\n",
    "\n",
    "end = time.time()\n",
    "total = end - start\n",
    "# print('the difference is',np.count_nonzero(weight_numpy-arn_weight))\n",
    "print('need {:.6f} sec in python'.format(total))\n",
    "\n",
    "model.features[3].weight.data[:64,:64] = torch.from_numpy(arn_weight).cuda()\n",
    "\n",
    "validate(val_loader, model, criterion)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need 0.007604 sec in python\n",
      "Test: [  0/196]\tTime  3.625 ( 3.625)\tLoss 1.1817e+00 (1.1817e+00)\tAcc@1  68.75 ( 68.75)\tAcc@5  89.06 ( 89.06)\n",
      "Test: [ 10/196]\tTime  0.020 ( 0.939)\tLoss 1.8619e+00 (1.5258e+00)\tAcc@1  53.91 ( 64.49)\tAcc@5  80.08 ( 84.23)\n",
      "Test: [ 20/196]\tTime  1.842 ( 0.922)\tLoss 1.4823e+00 (1.4910e+00)\tAcc@1  67.19 ( 65.23)\tAcc@5  83.20 ( 84.77)\n",
      "Test: [ 30/196]\tTime  0.021 ( 0.883)\tLoss 1.3381e+00 (1.4429e+00)\tAcc@1  62.89 ( 66.15)\tAcc@5  89.45 ( 85.37)\n",
      "Test: [ 40/196]\tTime  0.717 ( 0.848)\tLoss 1.6524e+00 (1.5115e+00)\tAcc@1  57.81 ( 63.71)\tAcc@5  84.38 ( 84.90)\n",
      "Test: [ 50/196]\tTime  0.021 ( 0.862)\tLoss 1.0528e+00 (1.4948e+00)\tAcc@1  71.48 ( 63.26)\tAcc@5  92.58 ( 85.52)\n",
      "Test: [ 60/196]\tTime  0.021 ( 0.839)\tLoss 1.8547e+00 (1.4879e+00)\tAcc@1  54.30 ( 63.28)\tAcc@5  81.64 ( 85.79)\n",
      "Test: [ 70/196]\tTime  0.021 ( 0.861)\tLoss 1.5965e+00 (1.4681e+00)\tAcc@1  60.55 ( 63.75)\tAcc@5  84.77 ( 86.03)\n",
      "Test: [ 80/196]\tTime  0.021 ( 0.842)\tLoss 2.7413e+00 (1.4955e+00)\tAcc@1  41.02 ( 63.38)\tAcc@5  66.41 ( 85.62)\n",
      "Test: [ 90/196]\tTime  0.021 ( 0.855)\tLoss 3.1913e+00 (1.5896e+00)\tAcc@1  34.77 ( 61.77)\tAcc@5  61.33 ( 84.19)\n",
      "Test: [100/196]\tTime  0.037 ( 0.836)\tLoss 2.8773e+00 (1.6689e+00)\tAcc@1  36.72 ( 60.31)\tAcc@5  63.67 ( 83.03)\n",
      "Test: [110/196]\tTime  0.021 ( 0.849)\tLoss 1.7661e+00 (1.7036e+00)\tAcc@1  61.72 ( 59.78)\tAcc@5  78.52 ( 82.42)\n",
      "Test: [120/196]\tTime  0.021 ( 0.833)\tLoss 2.7229e+00 (1.7423e+00)\tAcc@1  42.58 ( 59.23)\tAcc@5  64.84 ( 81.78)\n",
      "Test: [130/196]\tTime  0.021 ( 0.843)\tLoss 1.7960e+00 (1.7856e+00)\tAcc@1  56.25 ( 58.43)\tAcc@5  79.30 ( 81.13)\n",
      "Test: [140/196]\tTime  0.021 ( 0.826)\tLoss 2.2469e+00 (1.8191e+00)\tAcc@1  48.05 ( 57.92)\tAcc@5  72.66 ( 80.56)\n",
      "Test: [150/196]\tTime  0.021 ( 0.836)\tLoss 2.5929e+00 (1.8588e+00)\tAcc@1  53.52 ( 57.43)\tAcc@5  70.70 ( 79.92)\n",
      "Test: [160/196]\tTime  0.022 ( 0.826)\tLoss 1.8736e+00 (1.8887e+00)\tAcc@1  57.42 ( 56.93)\tAcc@5  78.12 ( 79.41)\n",
      "Test: [170/196]\tTime  0.021 ( 0.835)\tLoss 1.2882e+00 (1.9154e+00)\tAcc@1  69.14 ( 56.46)\tAcc@5  88.67 ( 79.03)\n",
      "Test: [180/196]\tTime  0.020 ( 0.824)\tLoss 1.8957e+00 (1.9347e+00)\tAcc@1  49.61 ( 56.10)\tAcc@5  79.30 ( 78.71)\n",
      "Test: [190/196]\tTime  0.021 ( 0.831)\tLoss 1.6515e+00 (1.9248e+00)\tAcc@1  54.69 ( 56.22)\tAcc@5  85.55 ( 78.89)\n",
      " * Acc@1 56.520 Acc@5 79.068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(56.5200, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_i = time.time()\n",
    "\n",
    "i_weight = iarnold(arn_weight, [1, 1,1,1,2])\n",
    "\n",
    "end_i = time.time()\n",
    "total_i = end_i - start_i\n",
    "\n",
    "print('need {:.6f} sec in python'.format(total_i))\n",
    "model.features[3].weight.data[:64,:64] = torch.from_numpy(i_weight).cuda()\n",
    "\n",
    "validate(val_loader, model, criterion)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-torch1.10.2]",
   "language": "python",
   "name": "conda-env-.conda-torch1.10.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
